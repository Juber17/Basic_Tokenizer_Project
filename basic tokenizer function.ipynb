{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Tokenizer Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tokenizer is a function that breaks up a string of text into smaller components, such as words or subwords. Here's a basic Python implementation of a tokenizer that splits a sentence based on spaces, punctuation, and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, world!\n",
      "Tokens: ['hello', 'world']\n",
      "\n",
      "Original: This is a basic tokenizer example.\n",
      "Tokens: ['this', 'is', 'a', 'basic', 'tokenizer', 'example']\n",
      "\n",
      "Original: Tokenize, split, and analyze data easily!\n",
      "Tokens: ['tokenize', 'split', 'and', 'analyze', 'data', 'easily']\n",
      "\n",
      "Original: It supports numbers like 123 and symbols like @ and #.Hello, world! This is a test of the basic tokenizerTokenizing can be quite useful in natural language processing (NLP)Let's see how this works on different sentencesHow about some complex sentences, such as this one, with commas, periods, and hyphens?12345 are numbers,Don’t forget about special characters: @#$%&*\n",
      "Tokens: ['it', 'supports', 'numbers', 'like', '123', 'and', 'symbols', 'like', 'and', 'hello', 'world', 'this', 'is', 'a', 'test', 'of', 'the', 'basic', 'tokenizertokenizing', 'can', 'be', 'quite', 'useful', 'in', 'natural', 'language', 'processing', 'nlp', 'let', 's', 'see', 'how', 'this', 'works', 'on', 'different', 'sentenceshow', 'about', 'some', 'complex', 'sentences', 'such', 'as', 'this', 'one', 'with', 'commas', 'periods', 'and', 'hyphens', '12345', 'are', 'numbers', 'don', 't', 'forget', 'about', 'special', 'characters']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def basic_tokenizer(text):\n",
    "    # Define a regular expression pattern for splitting on non-word characters (punctuation, spaces, etc.)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Test with some data\n",
    "data = [\n",
    "    \"Hello, world!\",\n",
    "    \"This is a basic tokenizer example.\",\n",
    "    \"Tokenize, split, and analyze data easily!\",\n",
    "    \"It supports numbers like 123 and symbols like @ and #.\"\n",
    "    \"Hello, world! This is a test of the basic tokenizer\"\n",
    "    \"Tokenizing can be quite useful in natural language processing (NLP)\"\n",
    "    \"Let's see how this works on different sentences\"\n",
    "    \"How about some complex sentences, such as this one, with commas, periods, and hyphens?\"\n",
    "    \"12345 are numbers,Don’t forget about special characters: @#$%&*\"\n",
    "\n",
    "]\n",
    "\n",
    "# Apply tokenizer to each sentence\n",
    "for sentence in data:\n",
    "    tokens = basic_tokenizer(sentence)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Tokens: {tokens}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Access to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileNotFoundError: The file 'data/sample_text.txt' was not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Tokenizer function\n",
    "def basic_tokenizer(text):\n",
    "    \"\"\"Tokenizes input text by splitting on words and non-whitespace characters.\"\"\"\n",
    "    tokens = re.findall(r'\\w+|\\S', text)\n",
    "    return tokens\n",
    "\n",
    "# Function to load data from a file\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads text data from the specified file.\n",
    "    \n",
    "    :param file_path: Path to the text file to read.\n",
    "    :return: The content of the file as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FileNotFoundError: The file '{file_path}' was not found.\")\n",
    "    except IOError:\n",
    "        print(f\"IOError: An error occurred while trying to read the file '{file_path}'.\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path to your dataset\n",
    "    file_path = 'data/sample_text.txt'  # Path to your sample text file\n",
    "\n",
    "    # Load the data\n",
    "    text_data = load_data(file_path)\n",
    "    \n",
    "    # Check if data was loaded successfully\n",
    "    if text_data:\n",
    "        # Tokenize the loaded data\n",
    "        tokens = basic_tokenizer(text_data)\n",
    "        # Print the tokens\n",
    "        print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
